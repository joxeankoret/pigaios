       #------------------------------------------------------------------------------
       # pycparser: c_lexer.py
       #
       # CLexer class: lexer for the C language
       #
       # Eli Bendersky [http://eli.thegreenplace.net]
       # License: BSD
       #------------------------------------------------------------------------------
    1: import re
    1: import sys
       
    1: from .ply import lex
    1: from .ply.lex import TOKEN
       
       
    2: class CLexer(object):
           """ A lexer for the C language. After building it, set the
               input text with input(), and call token() to get new
               tokens.
       
               The public attribute filename can be set to an initial
               filaneme, but the lexer will update it upon #line
               directives.
    1:     """
    1:     def __init__(self, error_func, on_lbrace_func, on_rbrace_func,
                        type_lookup_func):
               """ Create a new Lexer.
       
                   error_func:
                       An error function. Will be called with an error
                       message, line and column as arguments, in case of
                       an error during lexing.
       
                   on_lbrace_func, on_rbrace_func:
                       Called when an LBRACE or RBRACE is encountered
                       (likely to push/pop type_lookup_func's scope)
       
                   type_lookup_func:
                       A type lookup function. Given a string, it must
                       return True IFF this string is a name of a type
                       that was defined with a typedef earlier.
               """
               self.error_func = error_func
               self.on_lbrace_func = on_lbrace_func
               self.on_rbrace_func = on_rbrace_func
               self.type_lookup_func = type_lookup_func
               self.filename = ''
       
               # Keeps track of the last token returned from self.token()
               self.last_token = None
       
               # Allow either "# line" or "# <num>" to support GCC's
               # cpp output
               #
               self.line_pattern = re.compile(r'([ \t]*line\W)|([ \t]*\d+)')
               self.pragma_pattern = re.compile(r'[ \t]*pragma\W')
       
    1:     def build(self, **kwargs):
               """ Builds the lexer from the specification. Must be
                   called after the lexer object is created.
       
                   This method exists separately, because the PLY
                   manual warns against calling lex.lex inside
                   __init__
               """
               self.lexer = lex.lex(object=self, **kwargs)
       
    1:     def reset_lineno(self):
               """ Resets the internal line number counter of the lexer.
               """
               self.lexer.lineno = 1
       
    1:     def input(self, text):
               self.lexer.input(text)
       
    1:     def token(self):
               self.last_token = self.lexer.token()
               return self.last_token
       
    1:     def find_tok_column(self, token):
               """ Find the column of the token in its line.
               """
               last_cr = self.lexer.lexdata.rfind('\n', 0, token.lexpos)
               return token.lexpos - last_cr
       
           ######################--   PRIVATE   --######################
       
           ##
           ## Internal auxiliary methods
           ##
    1:     def _error(self, msg, token):
               location = self._make_tok_location(token)
               self.error_func(msg, location[0], location[1])
               self.lexer.skip(1)
       
    1:     def _make_tok_location(self, token):
               return (token.lineno, self.find_tok_column(token))
       
           ##
           ## Reserved keywords
           ##
           keywords = (
               '_BOOL', '_COMPLEX', 'AUTO', 'BREAK', 'CASE', 'CHAR', 'CONST',
               'CONTINUE', 'DEFAULT', 'DO', 'DOUBLE', 'ELSE', 'ENUM', 'EXTERN',
               'FLOAT', 'FOR', 'GOTO', 'IF', 'INLINE', 'INT', 'LONG',
               'REGISTER', 'OFFSETOF',
               'RESTRICT', 'RETURN', 'SHORT', 'SIGNED', 'SIZEOF', 'STATIC', 'STRUCT',
               'SWITCH', 'TYPEDEF', 'UNION', 'UNSIGNED', 'VOID',
    1:         'VOLATILE', 'WHILE', '__INT128',
           )
       
    1:     keyword_map = {}
   39:     for keyword in keywords:
   38:         if keyword == '_BOOL':
    1:             keyword_map['_Bool'] = keyword
   37:         elif keyword == '_COMPLEX':
    1:             keyword_map['_Complex'] = keyword
               else:
   36:             keyword_map[keyword.lower()] = keyword
       
           ##
           ## All the tokens recognized by the lexer
           ##
    1:     tokens = keywords + (
               # Identifiers
               'ID',
       
               # Type identifiers (identifiers previously defined as
               # types with typedef)
               'TYPEID',
       
               # constants
               'INT_CONST_DEC', 'INT_CONST_OCT', 'INT_CONST_HEX', 'INT_CONST_BIN',
               'FLOAT_CONST', 'HEX_FLOAT_CONST',
               'CHAR_CONST',
               'WCHAR_CONST',
       
               # String literals
               'STRING_LITERAL',
               'WSTRING_LITERAL',
       
               # Operators
               'PLUS', 'MINUS', 'TIMES', 'DIVIDE', 'MOD',
               'OR', 'AND', 'NOT', 'XOR', 'LSHIFT', 'RSHIFT',
               'LOR', 'LAND', 'LNOT',
               'LT', 'LE', 'GT', 'GE', 'EQ', 'NE',
       
               # Assignment
               'EQUALS', 'TIMESEQUAL', 'DIVEQUAL', 'MODEQUAL',
               'PLUSEQUAL', 'MINUSEQUAL',
               'LSHIFTEQUAL','RSHIFTEQUAL', 'ANDEQUAL', 'XOREQUAL',
               'OREQUAL',
       
               # Increment/decrement
               'PLUSPLUS', 'MINUSMINUS',
       
               # Structure dereference (->)
               'ARROW',
       
               # Conditional operator (?)
               'CONDOP',
       
               # Delimeters
               'LPAREN', 'RPAREN',         # ( )
               'LBRACKET', 'RBRACKET',     # [ ]
               'LBRACE', 'RBRACE',         # { }
               'COMMA', 'PERIOD',          # . ,
               'SEMI', 'COLON',            # ; :
       
               # Ellipsis (...)
               'ELLIPSIS',
       
               # pre-processor
               'PPHASH',       # '#'
               'PPPRAGMA',     # 'pragma'
    1:         'PPPRAGMASTR',
           )
       
           ##
           ## Regexes for use in tokens
           ##
           ##
       
           # valid C identifiers (K&R2: A.2.3), plus '$' (supported by some compilers)
    1:     identifier = r'[a-zA-Z_$][0-9a-zA-Z_$]*'
       
    1:     hex_prefix = '0[xX]'
    1:     hex_digits = '[0-9a-fA-F]+'
    1:     bin_prefix = '0[bB]'
    1:     bin_digits = '[01]+'
       
           # integer constants (K&R2: A.2.5.1)
    1:     integer_suffix_opt = r'(([uU]ll)|([uU]LL)|(ll[uU]?)|(LL[uU]?)|([uU][lL])|([lL][uU]?)|[uU])?'
    1:     decimal_constant = '(0'+integer_suffix_opt+')|([1-9][0-9]*'+integer_suffix_opt+')'
    1:     octal_constant = '0[0-7]*'+integer_suffix_opt
    1:     hex_constant = hex_prefix+hex_digits+integer_suffix_opt
    1:     bin_constant = bin_prefix+bin_digits+integer_suffix_opt
       
    1:     bad_octal_constant = '0[0-7]*[89]'
       
           # character constants (K&R2: A.2.5.2)
           # Note: a-zA-Z and '.-~^_!=&;,' are allowed as escape chars to support #line
           # directives with Windows paths as filenames (..\..\dir\file)
           # For the same reason, decimal_escape allows all digit sequences. We want to
           # parse all correct code, even if it means to sometimes parse incorrect
           # code.
           #
    1:     simple_escape = r"""([a-zA-Z._~!=&\^\-\\?'"])"""
    1:     decimal_escape = r"""(\d+)"""
    1:     hex_escape = r"""(x[0-9a-fA-F]+)"""
    1:     bad_escape = r"""([\\][^a-zA-Z._~^!=&\^\-\\?'"x0-7])"""
       
    1:     escape_sequence = r"""(\\("""+simple_escape+'|'+decimal_escape+'|'+hex_escape+'))'
    1:     cconst_char = r"""([^'\\\n]|"""+escape_sequence+')'
    1:     char_const = "'"+cconst_char+"'"
    1:     wchar_const = 'L'+char_const
    1:     unmatched_quote = "('"+cconst_char+"*\\n)|('"+cconst_char+"*$)"
    1:     bad_char_const = r"""('"""+cconst_char+"""[^'\n]+')|('')|('"""+bad_escape+r"""[^'\n]*')"""
       
           # string literals (K&R2: A.2.6)
    1:     string_char = r"""([^"\\\n]|"""+escape_sequence+')'
    1:     string_literal = '"'+string_char+'*"'
    1:     wstring_literal = 'L'+string_literal
    1:     bad_string_literal = '"'+string_char+'*?'+bad_escape+string_char+'*"'
       
           # floating constants (K&R2: A.2.5.3)
    1:     exponent_part = r"""([eE][-+]?[0-9]+)"""
    1:     fractional_constant = r"""([0-9]*\.[0-9]+)|([0-9]+\.)"""
    1:     floating_constant = '(((('+fractional_constant+')'+exponent_part+'?)|([0-9]+'+exponent_part+'))[FfLl]?)'
    1:     binary_exponent_part = r'''([pP][+-]?[0-9]+)'''
    1:     hex_fractional_constant = '((('+hex_digits+r""")?\."""+hex_digits+')|('+hex_digits+r"""\.))"""
    1:     hex_floating_constant = '('+hex_prefix+'('+hex_digits+'|'+hex_fractional_constant+')'+binary_exponent_part+'[FfLl]?)'
       
           ##
           ## Lexer states: used for preprocessor \n-terminated directives
           ##
           states = (
               # ppline: preprocessor line directives
               #
    1:         ('ppline', 'exclusive'),
       
               # pppragma: pragma
               #
    1:         ('pppragma', 'exclusive'),
           )
       
    1:     def t_PPHASH(self, t):
               r'[ \t]*\#'
               if self.line_pattern.match(t.lexer.lexdata, pos=t.lexer.lexpos):
                   t.lexer.begin('ppline')
                   self.pp_line = self.pp_filename = None
               elif self.pragma_pattern.match(t.lexer.lexdata, pos=t.lexer.lexpos):
                   t.lexer.begin('pppragma')
               else:
                   t.type = 'PPHASH'
                   return t
       
           ##
           ## Rules for the ppline state
           ##
    1:     @TOKEN(string_literal)
           def t_ppline_FILENAME(self, t):
               if self.pp_line is None:
                   self._error('filename before line number in #line', t)
               else:
                   self.pp_filename = t.value.lstrip('"').rstrip('"')
       
    1:     @TOKEN(decimal_constant)
           def t_ppline_LINE_NUMBER(self, t):
               if self.pp_line is None:
                   self.pp_line = t.value
               else:
                   # Ignore: GCC's cpp sometimes inserts a numeric flag
                   # after the file name
                   pass
       
    1:     def t_ppline_NEWLINE(self, t):
               r'\n'
               if self.pp_line is None:
                   self._error('line number missing in #line', t)
               else:
                   self.lexer.lineno = int(self.pp_line)
       
                   if self.pp_filename is not None:
                       self.filename = self.pp_filename
       
               t.lexer.begin('INITIAL')
       
    1:     def t_ppline_PPLINE(self, t):
               r'line'
               pass
       
    1:     t_ppline_ignore = ' \t'
       
    1:     def t_ppline_error(self, t):
               self._error('invalid #line directive', t)
       
           ##
           ## Rules for the pppragma state
           ##
    1:     def t_pppragma_NEWLINE(self, t):
               r'\n'
               t.lexer.lineno += 1
               t.lexer.begin('INITIAL')
       
    1:     def t_pppragma_PPPRAGMA(self, t):
               r'pragma'
               return t
       
    1:     t_pppragma_ignore = ' \t'
       
    1:     def t_pppragma_STR(self, t):
               '.+'
               t.type = 'PPPRAGMASTR'
               return t
       
    1:     def t_pppragma_error(self, t):
               self._error('invalid #pragma directive', t)
       
           ##
           ## Rules for the normal state
           ##
    1:     t_ignore = ' \t'
       
           # Newlines
    1:     def t_NEWLINE(self, t):
               r'\n+'
               t.lexer.lineno += t.value.count("\n")
       
           # Operators
    1:     t_PLUS              = r'\+'
    1:     t_MINUS             = r'-'
    1:     t_TIMES             = r'\*'
    1:     t_DIVIDE            = r'/'
    1:     t_MOD               = r'%'
    1:     t_OR                = r'\|'
    1:     t_AND               = r'&'
    1:     t_NOT               = r'~'
    1:     t_XOR               = r'\^'
    1:     t_LSHIFT            = r'<<'
    1:     t_RSHIFT            = r'>>'
    1:     t_LOR               = r'\|\|'
    1:     t_LAND              = r'&&'
    1:     t_LNOT              = r'!'
    1:     t_LT                = r'<'
    1:     t_GT                = r'>'
    1:     t_LE                = r'<='
    1:     t_GE                = r'>='
    1:     t_EQ                = r'=='
    1:     t_NE                = r'!='
       
           # Assignment operators
    1:     t_EQUALS            = r'='
    1:     t_TIMESEQUAL        = r'\*='
    1:     t_DIVEQUAL          = r'/='
    1:     t_MODEQUAL          = r'%='
    1:     t_PLUSEQUAL         = r'\+='
    1:     t_MINUSEQUAL        = r'-='
    1:     t_LSHIFTEQUAL       = r'<<='
    1:     t_RSHIFTEQUAL       = r'>>='
    1:     t_ANDEQUAL          = r'&='
    1:     t_OREQUAL           = r'\|='
    1:     t_XOREQUAL          = r'\^='
       
           # Increment/decrement
    1:     t_PLUSPLUS          = r'\+\+'
    1:     t_MINUSMINUS        = r'--'
       
           # ->
    1:     t_ARROW             = r'->'
       
           # ?
    1:     t_CONDOP            = r'\?'
       
           # Delimeters
    1:     t_LPAREN            = r'\('
    1:     t_RPAREN            = r'\)'
    1:     t_LBRACKET          = r'\['
    1:     t_RBRACKET          = r'\]'
    1:     t_COMMA             = r','
    1:     t_PERIOD            = r'\.'
    1:     t_SEMI              = r';'
    1:     t_COLON             = r':'
    1:     t_ELLIPSIS          = r'\.\.\.'
       
           # Scope delimiters
           # To see why on_lbrace_func is needed, consider:
           #   typedef char TT;
           #   void foo(int TT) { TT = 10; }
           #   TT x = 5;
           # Outside the function, TT is a typedef, but inside (starting and ending
           # with the braces) it's a parameter.  The trouble begins with yacc's
           # lookahead token.  If we open a new scope in brace_open, then TT has
           # already been read and incorrectly interpreted as TYPEID.  So, we need
           # to open and close scopes from within the lexer.
           # Similar for the TT immediately outside the end of the function.
           #
    1:     @TOKEN(r'\{')
           def t_LBRACE(self, t):
               self.on_lbrace_func()
               return t
    1:     @TOKEN(r'\}')
           def t_RBRACE(self, t):
               self.on_rbrace_func()
               return t
       
    1:     t_STRING_LITERAL = string_literal
       
           # The following floating and integer constants are defined as
           # functions to impose a strict order (otherwise, decimal
           # is placed before the others because its regex is longer,
           # and this is bad)
           #
    1:     @TOKEN(floating_constant)
           def t_FLOAT_CONST(self, t):
               return t
       
    1:     @TOKEN(hex_floating_constant)
           def t_HEX_FLOAT_CONST(self, t):
               return t
       
    1:     @TOKEN(hex_constant)
           def t_INT_CONST_HEX(self, t):
               return t
       
    1:     @TOKEN(bin_constant)
           def t_INT_CONST_BIN(self, t):
               return t
       
    1:     @TOKEN(bad_octal_constant)
           def t_BAD_CONST_OCT(self, t):
               msg = "Invalid octal constant"
               self._error(msg, t)
       
    1:     @TOKEN(octal_constant)
           def t_INT_CONST_OCT(self, t):
               return t
       
    1:     @TOKEN(decimal_constant)
           def t_INT_CONST_DEC(self, t):
               return t
       
           # Must come before bad_char_const, to prevent it from
           # catching valid char constants as invalid
           #
    1:     @TOKEN(char_const)
           def t_CHAR_CONST(self, t):
               return t
       
    1:     @TOKEN(wchar_const)
           def t_WCHAR_CONST(self, t):
               return t
       
    1:     @TOKEN(unmatched_quote)
           def t_UNMATCHED_QUOTE(self, t):
               msg = "Unmatched '"
               self._error(msg, t)
       
    1:     @TOKEN(bad_char_const)
           def t_BAD_CHAR_CONST(self, t):
               msg = "Invalid char constant %s" % t.value
               self._error(msg, t)
       
    1:     @TOKEN(wstring_literal)
           def t_WSTRING_LITERAL(self, t):
               return t
       
           # unmatched string literals are caught by the preprocessor
       
    1:     @TOKEN(bad_string_literal)
           def t_BAD_STRING_LITERAL(self, t):
               msg = "String contains invalid escape code"
               self._error(msg, t)
       
    1:     @TOKEN(identifier)
           def t_ID(self, t):
               t.type = self.keyword_map.get(t.value, "ID")
               if t.type == 'ID' and self.type_lookup_func(t.value):
                   t.type = "TYPEID"
               return t
       
    1:     def t_error(self, t):
               msg = 'Illegal character %s' % repr(t.value[0])
               self._error(msg, t)
